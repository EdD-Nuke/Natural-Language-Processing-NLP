{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Word Representations and Lexical Similarities\n",
    "\n",
    "This part has 18 points in total.\n",
    "\n",
    "Here we will compare different measures of semantic similarity between words: (1) WordNet depth distance (2) cosine similarity of words using a given GloVe model and (3) Resnet50 image features.\n",
    "\n",
    "For more reading on vector semantics got to Chapter 6, sections 6.4 and 6.8:\n",
    "https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "\n",
    "To learn about Wordnet: https://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "For additional Wordnet discussions see Chapter 19: https://web.stanford.edu/~jurafsky/slp3/19.pdf\n",
    "\n",
    "The GloVe word embeddings are described in [this paper](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "Resnet50: Deep Residual Learning for Image Recognition are described in [this paper](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "## Part 3.1: Semantic similarity with WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# load word-vector glov\n",
    "import gensim.downloader as gensim_api\n",
    "glove_model = gensim_api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "from itertools import combinations, product\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_words = ['car', 'dog', 'banana', 'delicious', 'baguette', 'jumping', 'hugging', 'election']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Word Representations in English WordNet (+3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Word: car ---\n",
      "Synset: car.n.01\n",
      "LEmmas: car, auto, automobile, machine, motorcar\n",
      "Hypernyms: motor_vehicle.n.01\n",
      "Hyponyms: ambulance.n.01, beach_wagon.n.01, bus.n.04, cab.n.03, compact.n.03, convertible.n.01, coupe.n.01, cruiser.n.01, electric.n.01, gas_guzzler.n.01, hardtop.n.01, hatchback.n.01, horseless_carriage.n.01, hot_rod.n.01, jeep.n.01, limousine.n.01, loaner.n.02, minicar.n.01, minivan.n.01, model_t.n.01, pace_car.n.01, racer.n.02, roadster.n.01, sedan.n.01, sport_utility.n.01, sports_car.n.01, stanley_steamer.n.01, stock_car.n.01, subcompact.n.01, touring_car.n.01, used-car.n.01\n",
      "\n",
      "____________________________\n",
      "Synset: car.n.02\n",
      "LEmmas: car, railcar, railway_car, railroad_car\n",
      "Hypernyms: wheeled_vehicle.n.01\n",
      "Hyponyms: baggage_car.n.01, cabin_car.n.01, club_car.n.01, freight_car.n.01, guard's_van.n.01, handcar.n.01, mail_car.n.01, passenger_car.n.01, slip_coach.n.01, tender.n.04, van.n.03\n",
      "\n",
      "____________________________\n",
      "Synset: car.n.03\n",
      "LEmmas: car, gondola\n",
      "Hypernyms: compartment.n.02\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: car.n.04\n",
      "LEmmas: car, elevator_car\n",
      "Hypernyms: compartment.n.02\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: cable_car.n.01\n",
      "LEmmas: cable_car, car\n",
      "Hypernyms: compartment.n.02\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "---Word: dog ---\n",
      "Synset: dog.n.01\n",
      "LEmmas: dog, domestic_dog, Canis_familiaris\n",
      "Hypernyms: canine.n.02, domestic_animal.n.01\n",
      "Hyponyms: basenji.n.01, corgi.n.01, cur.n.01, dalmatian.n.02, great_pyrenees.n.01, griffon.n.02, hunting_dog.n.01, lapdog.n.01, leonberg.n.01, mexican_hairless.n.01, newfoundland.n.01, pooch.n.01, poodle.n.01, pug.n.01, puppy.n.01, spitz.n.01, toy_dog.n.01, working_dog.n.01\n",
      "\n",
      "____________________________\n",
      "Synset: frump.n.01\n",
      "LEmmas: frump, dog\n",
      "Hypernyms: unpleasant_woman.n.01\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: dog.n.03\n",
      "LEmmas: dog\n",
      "Hypernyms: chap.n.01\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: cad.n.01\n",
      "LEmmas: cad, bounder, blackguard, dog, hound, heel\n",
      "Hypernyms: villain.n.01\n",
      "Hyponyms: perisher.n.01\n",
      "\n",
      "____________________________\n",
      "Synset: frank.n.02\n",
      "LEmmas: frank, frankfurter, hotdog, hot_dog, dog, wiener, wienerwurst, weenie\n",
      "Hypernyms: sausage.n.01\n",
      "Hyponyms: vienna_sausage.n.01\n",
      "\n",
      "____________________________\n",
      "Synset: pawl.n.01\n",
      "LEmmas: pawl, detent, click, dog\n",
      "Hypernyms: catch.n.06\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: andiron.n.01\n",
      "LEmmas: andiron, firedog, dog, dog-iron\n",
      "Hypernyms: support.n.10\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: chase.v.01\n",
      "LEmmas: chase, chase_after, trail, tail, tag, give_chase, dog, go_after, track\n",
      "Hypernyms: pursue.v.02\n",
      "Hyponyms: hound.v.01, quest.v.02, run_down.v.07, tree.v.03\n",
      "\n",
      "____________________________\n",
      "---Word: banana ---\n",
      "Synset: banana.n.01\n",
      "LEmmas: banana, banana_tree\n",
      "Hypernyms: herb.n.01\n",
      "Hyponyms: abaca.n.02, dwarf_banana.n.01, edible_banana.n.01, japanese_banana.n.01, plantain.n.02\n",
      "\n",
      "____________________________\n",
      "Synset: banana.n.02\n",
      "LEmmas: banana\n",
      "Hypernyms: edible_fruit.n.01\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "---Word: delicious ---\n",
      "Synset: delicious.n.01\n",
      "LEmmas: Delicious\n",
      "Hypernyms: eating_apple.n.01\n",
      "Hyponyms: golden_delicious.n.01, red_delicious.n.01\n",
      "\n",
      "____________________________\n",
      "Synset: delightful.s.01\n",
      "LEmmas: delightful, delicious\n",
      "Hypernyms: \n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: delectable.s.01\n",
      "LEmmas: delectable, delicious, luscious, pleasant-tasting, scrumptious, toothsome, yummy\n",
      "Hypernyms: \n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "---Word: baguette ---\n",
      "Synset: baguet.n.01\n",
      "LEmmas: baguet, baguette\n",
      "Hypernyms: french_bread.n.01\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "---Word: jumping ---\n",
      "Synset: jumping.n.01\n",
      "LEmmas: jumping\n",
      "Hypernyms: track_and_field.n.01\n",
      "Hyponyms: broad_jump.n.02, high_jump.n.02\n",
      "\n",
      "____________________________\n",
      "Synset: jump.n.06\n",
      "LEmmas: jump, jumping\n",
      "Hypernyms: propulsion.n.02\n",
      "Hyponyms: capriole.n.01, header.n.07, hop.n.01, jumping_up_and_down.n.01, leap.n.01, vault.n.04\n",
      "\n",
      "____________________________\n",
      "Synset: jump.v.01\n",
      "LEmmas: jump, leap, bound, spring\n",
      "Hypernyms: move.v.03\n",
      "Hyponyms: bounce.v.01, bounce.v.05, burst.v.04, caper.v.01, capriole.v.01, curvet.v.01, galumph.v.01, hop.v.01, hop.v.06, leapfrog.v.01, pronk.v.01, saltate.v.02, ski_jump.v.01, vault.v.01, vault.v.02\n",
      "\n",
      "____________________________\n",
      "Synset: startle.v.02\n",
      "LEmmas: startle, jump, start\n",
      "Hypernyms: move.v.03\n",
      "Hyponyms: boggle.v.01, jackrabbit.v.01, rear_back.v.02, shy.v.01\n",
      "\n",
      "____________________________\n",
      "Synset: jump.v.03\n",
      "LEmmas: jump\n",
      "Hypernyms: assail.v.01\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: jump.v.04\n",
      "LEmmas: jump\n",
      "Hypernyms: wax.v.02\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: leap_out.v.01\n",
      "LEmmas: leap_out, jump_out, jump, stand_out, stick_out\n",
      "Hypernyms: look.v.02\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: jump.v.06\n",
      "LEmmas: jump\n",
      "Hypernyms: enter.v.02\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: rise.v.11\n",
      "LEmmas: rise, jump, climb_up\n",
      "Hypernyms: change.v.02\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: jump.v.08\n",
      "LEmmas: jump, leap, jump_off\n",
      "Hypernyms: move.v.03\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: derail.v.02\n",
      "LEmmas: derail, jump\n",
      "Hypernyms: travel.v.01\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: chute.v.01\n",
      "LEmmas: chute, parachute, jump\n",
      "Hypernyms: dive.v.01\n",
      "Hyponyms: sky_dive.v.01\n",
      "\n",
      "____________________________\n",
      "Synset: jump.v.11\n",
      "LEmmas: jump, leap\n",
      "Hypernyms: \n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: jumpstart.v.01\n",
      "LEmmas: jumpstart, jump-start, jump\n",
      "Hypernyms: start.v.08\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: jump.v.13\n",
      "LEmmas: jump, pass_over, skip, skip_over\n",
      "Hypernyms: neglect.v.01\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: leap.v.02\n",
      "LEmmas: leap, jump\n",
      "Hypernyms: switch.v.03\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: alternate.v.01\n",
      "LEmmas: alternate, jump\n",
      "Hypernyms: change.v.03\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "---Word: hugging ---\n",
      "Synset: caressing.n.01\n",
      "LEmmas: caressing, cuddling, fondling, hugging, kissing, necking, petting, smooching, snuggling\n",
      "Hypernyms: foreplay.n.01\n",
      "Hyponyms: snogging.n.01\n",
      "\n",
      "____________________________\n",
      "Synset: embrace.v.02\n",
      "LEmmas: embrace, hug, bosom, squeeze\n",
      "Hypernyms: clasp.v.01\n",
      "Hyponyms: clinch.v.04, cuddle.v.02, interlock.v.03\n",
      "\n",
      "____________________________\n",
      "Synset: hug.v.02\n",
      "LEmmas: hug\n",
      "Hypernyms: touch.v.05\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "---Word: election ---\n",
      "Synset: election.n.01\n",
      "LEmmas: election\n",
      "Hypernyms: vote.n.02\n",
      "Hyponyms: by-election.n.01, general_election.n.01, primary.n.01, reelection.n.01, runoff.n.02\n",
      "\n",
      "____________________________\n",
      "Synset: election.n.02\n",
      "LEmmas: election\n",
      "Hypernyms: choice.n.02\n",
      "Hyponyms: co-option.n.01, cumulative_vote.n.01\n",
      "\n",
      "____________________________\n",
      "Synset: election.n.03\n",
      "LEmmas: election\n",
      "Hypernyms: status.n.01\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n",
      "Synset: election.n.04\n",
      "LEmmas: election\n",
      "Hypernyms: predestination.n.02\n",
      "Hyponyms: \n",
      "\n",
      "____________________________\n"
     ]
    }
   ],
   "source": [
    "# For each word above print their synsets\n",
    "# for each synset print all lemmas, hypernyms, hyponyms\n",
    "\n",
    "for word in some_words:\n",
    "    synsets = wn.synsets(word)\n",
    "    print(f'---Word: {word} ---')\n",
    "    \n",
    "    for synset in synsets:\n",
    "        synset_name = synset.name()\n",
    "        print(f'Synset: {synset_name}')\n",
    "        \n",
    "        synset_info = wn.synset(synset_name)\n",
    "        lemmas = ', '.join(map(lambda lemma: lemma.name(), synset_info.lemmas()))\n",
    "        print(f'LEmmas: {lemmas}')\n",
    "        \n",
    "        hypernyms = ', '.join(map(lambda hypernym: hypernym.name(), synset_info.hypernyms()))\n",
    "        print(f'Hypernyms: {hypernyms}')\n",
    "        \n",
    "        \n",
    "        hyponyms = ', '.join(map(lambda hyponym: hyponym.name(), synset_info.hyponyms()))\n",
    "        print(f'Hyponyms: {hyponyms}\\n')\n",
    "        print(f'____________________________')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure The Lexical Similarity (+3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car       dog        0.667\n",
      "car       banana     0.421\n",
      "car       delicious  0.364\n",
      "car       baguette   0.211\n",
      "car       jumping    0.167\n",
      "car       hugging    0.235\n",
      "car       election   0.133\n",
      "dog       banana     0.632\n",
      "dog       delicious  0.556\n",
      "dog       baguette   0.556\n",
      "dog       jumping    0.333\n",
      "dog       hugging    0.286\n",
      "dog       election   0.182\n",
      "banana    delicious  0.750\n",
      "banana    baguette   0.556\n",
      "banana    jumping    0.167\n",
      "banana    hugging    0.250\n",
      "banana    election   0.143\n",
      "delicious baguette   0.500\n",
      "delicious jumping    0.500\n",
      "delicious hugging    0.400\n",
      "delicious election   0.222\n",
      "baguette  jumping    0.154\n",
      "baguette  hugging    0.222\n",
      "baguette  election   0.125\n",
      "jumping   hugging    0.400\n",
      "jumping   election   0.667\n",
      "hugging   election   0.200\n",
      "\n",
      "Most Similiar words\n",
      "Pair: ('banana', 'delicious'), Score: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Wu-Palmer Similarity is a measure of similarity between to sense based on their depth distance. \n",
    "#\n",
    "# For each pair of words, find their closes sense based on Wu-Palmer Similarity.\n",
    "# List all word pairs and their highest possible wup_similarity. \n",
    "# Use wn.wup_similarity(s1, s2) and itertools (combinations and product).\n",
    "# if there is no connection between two words, put 0.\n",
    "w_pairs = []\n",
    "wn_sims = []\n",
    "for word1, word2 in combinations(some_words, 2):\n",
    "    w_pairs.append((word1, word2))\n",
    "    w1_synsets = wn.synsets(word1)\n",
    "    w2_synsets = wn.synsets(word2)\n",
    "    \n",
    "    max_sim = 0\n",
    "    for synset1, synset2 in product(w1_synsets, w2_synsets):\n",
    "        sim = wn.wup_similarity(synset1, synset2)\n",
    "        \n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "    wn_sims.append(max_sim)\n",
    "    print(f\"{word1:9} {word2:9} {max_sim:6.3f}\")\n",
    "    ######################\n",
    "max_sim = max(wn_sims)\n",
    "max_sim_index = wn_sims.index(max_sim)\n",
    "max_sim_pair = w_pairs[max_sim_index]\n",
    "\n",
    "print(f'\\nMost Similiar words')\n",
    "##Pairing\n",
    "print(f'Pair: {max_sim_pair}, Score: {max_sim}')\n",
    "\n",
    "# which word pair are the most similar words?\n",
    "# Answear: banana    delicious  0.750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.2: Semantic similarity with GloVe and comparison with WordNet\n",
    "\n",
    "### Measure the similarities on GloVe Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car       dog        0.464\n",
      "car       banana     0.219\n",
      "car       delicious  0.068\n",
      "car       baguette   0.046\n",
      "car       jumping    0.516\n",
      "car       hugging    0.278\n",
      "car       election   0.333\n",
      "dog       banana     0.333\n",
      "dog       delicious  0.404\n",
      "dog       baguette   0.018\n",
      "dog       jumping    0.539\n",
      "dog       hugging    0.410\n",
      "dog       election   0.181\n",
      "banana    delicious  0.487\n",
      "banana    baguette   0.450\n",
      "banana    jumping    0.108\n",
      "banana    hugging    0.127\n",
      "banana    election   0.164\n",
      "delicious baguette   0.421\n",
      "delicious jumping    0.042\n",
      "delicious hugging    0.142\n",
      "delicious election   0.028\n",
      "baguette  jumping   -0.075\n",
      "baguette  hugging    0.161\n",
      "baguette  election  -0.091\n",
      "jumping   hugging    0.447\n",
      "jumping   election   0.206\n",
      "hugging   election  -0.076\n"
     ]
    }
   ],
   "source": [
    "glov_sims = []\n",
    "for word1, word2 in combinations(some_words, 2):\n",
    "    max_sim = glove_model.similarity(word1, word2)\n",
    "    glov_sims.append(max_sim)\n",
    "    print(f\"{word1:9} {word2:9} {max_sim:6.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine if two measures correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rho SpearmanrResult(correlation=0.4222499442309076, pvalue=0.02519986065189366)\n"
     ]
    }
   ],
   "source": [
    "# a correlation coefficent of two lists\n",
    "print(\"Spearman's rho\", spearmanr(glov_sims, wn_sims))\n",
    "\n",
    "# Higher correlation (closer to 1.0) means two measures agree with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the two similarities compare? (+3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "#There are diffrences with the two similarity models Glove and Wu. Glove_model uses cosine similarity, calculated\n",
    "#distance of vectors in the feature \"plane\"/space. The logic goes that if distance is 0 the vectors between the features\n",
    "#are 90 degrees, therefore no similarity. \n",
    "\n",
    "#Wu-Palmer uses depth of the hypernym tree and calculates the similarity between the words. Its almost like a BTS-tree\n",
    "#where it has a root. Therefore there is a common root for all words and it cant be negative. \n",
    "\n",
    "#The next segment explains it more etensively, dependencies/similarities are explained as Vectors using cosine_sim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector Representations in GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog = [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Each word is represented as a vector:\n",
    "print('dog =', glove_model['dog'])\n",
    "\n",
    "# matrix of all word vectors is trained as parameters of a language model:\n",
    "# P( target_word | context_word ) = f(word, context ; params)\n",
    "#\n",
    "# Words in a same sentence and in close proximity are in context of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Cosine Similarity (+3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6956217371771922"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on equation 6.10 J&M (2019)\n",
    "# https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "#\n",
    "def cosine_sim(v1, v2):\n",
    "    def dot(v1, v2):\n",
    "        return sum(v1*v2)\n",
    "    \n",
    "    def length(v):\n",
    "        return dot(v, v) ** 0.5\n",
    "    \n",
    "    return dot(v1, v2)/ (length(v1) * length(v2))\n",
    "\n",
    "cosine_sim(glove_model['car'], glove_model['automobile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement top-n most similar words (+3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to car: [('truck', 0.9208586111508545), ('cars', 0.8870189568390014), ('vehicle', 0.8833684160157461)]\n",
      "Most similar words to dog: [('cat', 0.9218005180563862), ('dogs', 0.851315860708305), ('horse', 0.7907583073303338)]\n",
      "Most similar words to banana: [('bananas', 0.8152027815146712), ('coconut', 0.7872509691547495), ('pineapple', 0.7579815027266895)]\n",
      "Most similar words to delicious: [('tasty', 0.9297150293689781), ('savory', 0.8695854940874334), ('spicy', 0.8472648526796178)]\n",
      "Most similar words to baguette: [('brioche', 0.7667232075941974), ('baguettes', 0.7605002107488397), ('focaccia', 0.7604009696564551)]\n",
      "Most similar words to jumping: [('jump', 0.8335474991521888), ('jumps', 0.7868058809019087), ('climbing', 0.7549090228759915)]\n",
      "Most similar words to hugging: [('kissed', 0.7903850508373103), ('kissing', 0.7728414628740475), ('hugged', 0.765109992226354)]\n",
      "Most similar words to election: [('elections', 0.9582603048189698), ('polls', 0.890477251299923), ('vote', 0.8788222677984862)]\n"
     ]
    }
   ],
   "source": [
    "# search in glove_model:\n",
    "def top_n(word, n):\n",
    "    # example: top_n('dog', 3) =  \n",
    "    #[('cat', 0.9218005537986755),\n",
    "    # ('dogs', 0.8513159155845642),\n",
    "    # ('horse', 0.7907583713531494)]\n",
    "    # similar to glove_model.most_similar('dog', topn=3)\n",
    "    sims = []\n",
    "    for sim_w in glove_model.index_to_key:\n",
    "        if sim_w == word:\n",
    "            continue\n",
    "            \n",
    "        vector_w = glove_model[word]\n",
    "        sim_vec_w = glove_model[sim_w]\n",
    "        \n",
    "        sim = cosine_sim(vector_w, sim_vec_w)\n",
    "        pair = sim_w, sim\n",
    "        sims.append(pair)\n",
    "    \n",
    "    sims.sort(key = lambda pair: pair[1], reverse = True)\n",
    "    out = sims[:n]\n",
    "    return out\n",
    "\n",
    "#To test most sim. w to dog\n",
    "for word in some_words:\n",
    "    print(f'Most similar words to {word}:', top_n(word, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.3: Semantic similarity with visual features (ResNet)\n",
    "\n",
    "\n",
    "### Measure the similarities with the ResNet vectors\n",
    "\n",
    "In this part we will use visual features of images representing these objects. If you are interested how we extract these features have a look at `visual-feature-extraction.ipynb` but understanding that notebook is not necessary to complete this part as we have saved them for you they are loaded in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the feature extractor on all images\n",
    "# make sure that the order of features is identical to the order of words (variable some_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car',\n",
       " 'dog',\n",
       " 'banana',\n",
       " 'delicious',\n",
       " 'baguette',\n",
       " 'jumping',\n",
       " 'hugging',\n",
       " 'election']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'car': 0, 'dog': 1, 'banana': 2, 'delicious': 3, 'baguette': 4, 'jumping': 5, 'hugging': 6, 'election': 7}\n"
     ]
    }
   ],
   "source": [
    "object_indices = {v:k for k,v in enumerate(some_words)}\n",
    "print(object_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "image_features = torch.load('image_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2048])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car       dog        0.779\n",
      "car       banana     0.749\n",
      "car       delicious  0.751\n",
      "car       baguette   0.738\n",
      "car       jumping    0.752\n",
      "car       hugging    0.734\n",
      "car       election   0.752\n",
      "dog       banana     0.770\n",
      "dog       delicious  0.786\n",
      "dog       baguette   0.769\n",
      "dog       jumping    0.763\n",
      "dog       hugging    0.803\n",
      "dog       election   0.802\n",
      "banana    delicious  0.791\n",
      "banana    baguette   0.776\n",
      "banana    jumping    0.749\n",
      "banana    hugging    0.751\n",
      "banana    election   0.735\n",
      "delicious baguette   0.778\n",
      "delicious jumping    0.787\n",
      "delicious hugging    0.748\n",
      "delicious election   0.758\n",
      "baguette  jumping    0.750\n",
      "baguette  hugging    0.731\n",
      "baguette  election   0.746\n",
      "jumping   hugging    0.765\n",
      "jumping   election   0.758\n",
      "hugging   election   0.785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "resnet_sims = []\n",
    "\n",
    "# Load the Resnet vectors and create a for loop to compare the words pairwise.\n",
    "\n",
    "# A loop that creates similarities for images pairwise TODO\n",
    "# for resnet\n",
    "for w1, w2 in combinations(some_words, 2):\n",
    "    w1_visfeat = image_features[object_indices[w1]].unsqueeze(0).detach().numpy()\n",
    "    w2_visfeat = image_features[object_indices[w2]].unsqueeze(0).detach().numpy()\n",
    "    max_sim = cosine_similarity(w1_visfeat, w2_visfeat)[0][0]\n",
    "    resnet_sims.append(max_sim)\n",
    "    print(f\"{w1:9} {w2:9} {max_sim:6.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine if Resnet and GloVe similarities correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rho SpearmanrResult(correlation=0.3459222769567597, pvalue=0.07136834743507675)\n"
     ]
    }
   ],
   "source": [
    "# a correlation coefficent of two lists\n",
    "print(\"Spearman's rho\", spearmanr(resnet_sims, glov_sims))\n",
    "\n",
    "# Higher correlation (closer to 1.0) means two measures agree with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does semantic similarity from word vectors compare with the visual similarity? Are there differences between different words? (+3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "#As we have seen there might not be so much of a clear similarity between some words that have\n",
    "#visual similarity but not word-similairty or vice versa. Example of visual similarites, \"delicious\" for\n",
    "#spaghetti might also be associated with banana. Here there are very diffrent visual similiarties but very\n",
    "#similar word similarities, i.e both are \"delicious\". (see jpg:s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.4 Optional: Examine Fairness In Data Driven Word Vectors\n",
    "\n",
    "There are no points for this part but you are welcome to further explore this topic if you are inetrested in it. We will address it again in the Computational semantics course.\n",
    "\n",
    "Caliskan et al. (2017) argues that word vectors learn human biases from data. \n",
    "\n",
    "Try to replicate one of the tests of the paper:\n",
    "\n",
    "Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. “Semantics derived automatically from language corpora contain human-like biases.” Science\n",
    "356.6334 (2017): 183-186. http://opus.bath.ac.uk/55288/\n",
    "\n",
    "\n",
    "For example on gender bias:\n",
    "- Male names: John, Paul, Mike, Kevin, Steve, Greg, Jeff, Bill.\n",
    "- Female names: Amy, Joan, Lisa, Sarah, Diana, Kate, Ann, Donna.\n",
    "- Career words : executive, management, professional, corporation, salary, office, business, career.\n",
    "- Family words : home, parents, children, family, cousins, marriage, wedding, relatives.\n",
    "\n",
    "\n",
    "Report the average cosine similarity of male names to career words, and compare it with the average similarity of female names to career words. (repeat for family words) \n",
    "\n",
    "tokens in GloVe model are all in lower case.\n",
    "\n",
    "Write at least one sentence to describe your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
