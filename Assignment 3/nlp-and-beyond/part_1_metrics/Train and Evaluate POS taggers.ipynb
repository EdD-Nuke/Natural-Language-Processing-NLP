{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Evaluate Multi-class Taggers with Precision, Recall, and F1-score \n",
    "\n",
    "This part has 15 points.\n",
    "\n",
    "Study this notebook: run the code which trains different classifiers and evaluates average performances with precision, recall and f1score. The code also reports the scores for each individual label in each model.\n",
    "\n",
    "Your task is to reimplement functions for accuracy_score (1pt), precision_score (3pt), recall_score (3pt), f1_score (2pt) with options for multiple labels and mico- and macro-averaging for them. The answer should be a simplified form of the function in scikit-learn ([precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html), [recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), [f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)). \n",
    "\n",
    "For details check out [Chapter 4 section 4.7](https://web.stanford.edu/~jurafsky/slp3/4.pdf) (Jurafsty and Martin; 2019).\n",
    "\n",
    "Add code to produce a confusion matrix (3pt)\n",
    "\n",
    "Explain the effect of the backoff (3pt)\n",
    "\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\edina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\edina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\edina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Brown corpus:\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# load a tagger models\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "# Naive Bayes MLE \n",
    "from nltk.tag.sequential import NgramTagger\n",
    "\n",
    "# tagset mapping:\n",
    "from nltk.tag.mapping import map_tag\n",
    "\n",
    "# plotting:\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# If you downloaded these corous before comment below lines\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# you can compare you implementation with these\n",
    "# evaluation metrics:\n",
    "from sklearn.metrics import (\n",
    "    f1_score as _f1_score, \n",
    "    precision_score as _precision_score, \n",
    "    recall_score as _recall_score,\n",
    "    accuracy_score as _accuracy_score\n",
    ")\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing data:\n",
    "test_train_split = 500\n",
    "test_set = brown.tagged_sents()[:test_train_split]\n",
    "train_set = brown.tagged_sents()[test_train_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training and testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or train the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-trained perceptron tagger:\n",
    "perceptron_tagger = PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train Naive Bayes / count-based ngram taggers:\n",
    "unigram_tagger = NgramTagger(1, train=train_set)\n",
    "bigram_tagger_nobackoff = NgramTagger(2, train=train_set)\n",
    "bigram_tagger = NgramTagger(2, train=train_set, backoff=unigram_tagger)\n",
    "trigram_tagger = NgramTagger(3, train=train_set, backoff=bigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Perceptron\": perceptron_tagger, \n",
    "    \"Unigram\": unigram_tagger, \n",
    "    \"Bigram\": bigram_tagger, \n",
    "    \"Trigram\": trigram_tagger, \n",
    "    \"Bigram-backoff\": bigram_tagger_nobackoff, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the models\n",
    "\n",
    "The test dataset and the models are based on the English Penn TreeBank tagsets. However, we don't need that fine degree of granularity. Therefore, we map each tag onto unviversal tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "# the ground truth labels according to the dataset:\n",
    "tags_true = [\n",
    "    map_tag(\"en-brown\", \"universal\", tag)\n",
    "    for tagged_sent in test_set\n",
    "    for word, tag in tagged_sent\n",
    "]\n",
    "\n",
    "# strip the tags:\n",
    "test_set_sents = [\n",
    "    [word for word, tag in tagged_sent]\n",
    "    for tagged_sent in test_set\n",
    "]\n",
    "\n",
    "tagset = sorted(list(set(tags_true)))\n",
    "print(tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true : 1d array-like Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like Estimated targets as returned by a classifier.\n",
    "    FORMULA:\n",
    "    ACC = correct / total\n",
    "    \"\"\"\n",
    "    corr_count = 0\n",
    "    tot = 0\n",
    "    maxim = len(y_true)\n",
    "    for i in range (maxim): #if find match for both true and pred incerement both else just increment max sum\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            corr_count += 1\n",
    "            tot += 1\n",
    "        else:\n",
    "            tot += 1\n",
    "    accuracy = corr_count/tot\n",
    "    # your code here\n",
    "    #\n",
    "    return accuracy# the overall accuracy score with exact match\n",
    "    \n",
    "def precision_score(y_true, y_pred, labels=None, average=None):\n",
    "    \"\"\"\n",
    "    y_true : 1d array-like Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like Estimated targets as returned by a classifier.\n",
    "    labels : list of unique labels for sort out the result\n",
    "    average : string, ['micro', 'macro'] instead of defining labels.\n",
    "    \n",
    "    When true positive + false positive == 0 returns 0\n",
    "    \"\"\"\n",
    "    maxim = len(y_true) #tot length of y_trues in order to later match them to tp or fp\n",
    "    vals = {}\n",
    "\n",
    "    for i in range(maxim):\n",
    "        tag = y_pred[i]\n",
    "        if tag not in vals:\n",
    "            vals[tag] = {'true_positive': 0, 'false_positive': 0} #label tag of y_pred in y_true set labels\n",
    "    \n",
    "    for i in range(0, maxim):\n",
    "        tag = y_pred[i]\n",
    "        tag_true = y_true[i] #for every i-th item withing the y_ture  and y_pred test each with each other\n",
    "\n",
    "        if tag == tag_true:\n",
    "            vals[tag]['true_positive'] = vals[tag]['true_positive']+1\n",
    "        elif tag != tag_true:\n",
    "            vals[tag]['false_positive'] = vals[tag]['false_positive']+1\n",
    "\n",
    "    if labels is None:\n",
    "        if average == 'micro':\n",
    "            #Mii matrix for dict with counts\n",
    "            mini_mat = {'true_positive': 0, 'false_positive': 0}\n",
    "            for key in vals:\n",
    "                mini_mat['true_positive'] = mini_mat['true_positive'] + vals[key]['true_positive']\n",
    "                mini_mat['false_positive'] = mini_mat['false_positive'] + vals[key]['false_positive']\n",
    "            tp_counter = mini_mat['true_positive']\n",
    "            fp_counter = mini_mat['false_positive']\n",
    "            zero_div = tp_counter + fp_counter\n",
    "            if zero_div == 0:\n",
    "                micro_precision = 0\n",
    "            else:\n",
    "                micro_precision = tp_counter / zero_div\n",
    "            return micro_precision # the score with micro averaging\n",
    "        elif average == 'macro':\n",
    "            #List List_prec --> Averaged\n",
    "            list_prec = []\n",
    "            for key in vals:\n",
    "                tp_counter = vals[key]['true_positive']\n",
    "                fp_counter = vals[key]['false_positive']\n",
    "                zero_div = tp_counter + fp_counter\n",
    "                if zero_div == 0:\n",
    "                    precision = 0\n",
    "                else:\n",
    "                    precision = tp_counter / zero_div\n",
    "                list_prec.append(precision)\n",
    "            #Now average out proc.\n",
    "            count_precs = 0\n",
    "            for precision in list_prec:\n",
    "                count_precs += precision\n",
    "            zero_div = len(list_prec)\n",
    "            macro_precision = count_precs / zero_div\n",
    "            return macro_precision # the score with micro averaging\n",
    "    else:\n",
    "        #\n",
    "        # your code here\n",
    "        #\n",
    "        prec_list = []\n",
    "        for key in labels:\n",
    "            tp_counter = vals[key]['true_positive']\n",
    "            fp_counter = vals[key]['false_positive']\n",
    "            zero_div = tp_counter + fp_counter\n",
    "            if zero_div == 0:\n",
    "                precision = 0\n",
    "            else:\n",
    "                precision = tp_counter / zero_div\n",
    "            prec_list.append(precision)\n",
    "\n",
    "        return prec_list# list of score of each label \n",
    "\n",
    "def recall_score(y_true, y_pred, labels=None, average=None):\n",
    "    \"\"\"\n",
    "    y_true : 1d array-like Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like Estimated targets as returned by a classifier.\n",
    "    labels : list of unique labels for sort out the result\n",
    "    average : string, ['micro', 'macro'] instead of defining labels.\n",
    "    \n",
    "    When true positive + false positive == 0 returns 0\n",
    "    \"\"\"\n",
    "\n",
    "    maxim = len(y_true)\n",
    "    vals = {}\n",
    "    #the rest of the structure below will follow the same pattern, assigning to tags with labels tp and fn\n",
    "    #comparing the tags and finally dealing with zero divsion issues, with zero_div\n",
    "    for i in range (0, maxim):\n",
    "        tag = y_pred[i]\n",
    "        if tag not in vals:\n",
    "            vals[tag] = {'true_positive': 0, 'false_negative': 0}\n",
    "    \n",
    "    for i in range (0, maxim):\n",
    "        tag = y_pred[i]\n",
    "        tag_true = y_true[i]\n",
    "\n",
    "        if tag == tag_true:\n",
    "            vals[tag]['true_positive'] = vals[tag]['true_positive'] + 1\n",
    "        elif tag != tag_true:\n",
    "            vals[tag_true]['false_negative'] = vals[tag_true]['false_negative'] + 1\n",
    "\n",
    "\n",
    "    if labels is None:\n",
    "        if average == 'micro':\n",
    "            #create mini matrix for tp and fn aka sum(tp + fn) but tags\n",
    "            mat = {'true_positive': 0, 'false_negative': 0}\n",
    "            for key in vals:\n",
    "                mat['true_positive'] = mat['true_positive'] + vals[key]['true_positive']\n",
    "                mat['false_negative'] = mat['false_negative'] + vals[key]['false_negative']\n",
    "            tp_counter = mat['true_positive']\n",
    "            fn_counter = mat['false_negative']\n",
    "            zero_div = tp_counter + fn_counter\n",
    "            if zero_div == 0:\n",
    "                micro_recall = 0\n",
    "            else:\n",
    "                micro_recall = tp_counter / zero_div\n",
    "            return micro_recall # the score with micro averaging\n",
    "        elif average == 'macro':\n",
    "            comp_list_rec = []\n",
    "            for key in vals:\n",
    "                tp_counter = vals[key]['true_positive']\n",
    "                fn_counter = vals[key]['false_negative']\n",
    "                zero_div = tp_counter + fn_counter\n",
    "                if zero_div == 0:\n",
    "                    m_recall = 0\n",
    "                else:\n",
    "                    m_recall = tp_counter / zero_div\n",
    "                comp_list_rec.append(m_recall)\n",
    "            count_recs = 0\n",
    "            for m_recall in comp_list_rec:\n",
    "                count_recs += m_recall\n",
    "            zero_div = len(comp_list_rec)\n",
    "            macro_recall = count_recs / zero_div\n",
    "            #\n",
    "            # your code here\n",
    "            #\n",
    "            return macro_recall # the score with macro averaging\n",
    "    else:\n",
    "        #\n",
    "        # your code here\n",
    "        #\n",
    "        recs_list = []\n",
    "        for key in labels:\n",
    "            tp_counter = vals[key]['true_positive']\n",
    "            fn_counter = vals[key]['false_negative']\n",
    "            zero_div = tp_counter + fn_counter\n",
    "            if zero_div == 0:\n",
    "                recall = 0\n",
    "            else:\n",
    "                recall = tp_counter / zero_div\n",
    "            recs_list.append(recall)\n",
    "        return recs_list # list of score of each label \n",
    "\n",
    "def f1_score(y_true, y_pred, labels=None, average=None):\n",
    "    \"\"\"\n",
    "    y_true : 1d array-like Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like Estimated targets as returned by a classifier.\n",
    "    labels : list of unique labels for sort out the result\n",
    "    average : string, ['micro', 'macro'] instead of defining labels.\n",
    "\n",
    "    The F1 score has its best score at one and bad value at 0.\n",
    "    Formula for F1:\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \"\"\"\n",
    "    # you can call recall_score and precision_score.\n",
    "    if labels is None:\n",
    "        if average == 'micro':\n",
    "            #\n",
    "            micro_pre = precision_score(y_true, y_pred, labels=None, average='micro')\n",
    "            micro_rec = recall_score(y_true, y_pred, labels=None, average='micro')\n",
    "            zero_div = micro_pre + micro_rec\n",
    "            if zero_div == 0:\n",
    "                f1 = 0\n",
    "            else:\n",
    "                f1 = (2 * (micro_pre * micro_rec) / zero_div)\n",
    "\n",
    "            return f1   # the score with micro averaging\n",
    "        \n",
    "        elif average == 'macro':\n",
    "            # your code here\n",
    "            macro_pre = precision_score(y_true, y_pred, labels=None, average='macro')\n",
    "            macro_rec = recall_score(y_true, y_pred, labels=None, average='macro')\n",
    "            zero_div = macro_pre + macro_rec\n",
    "            if zero_div == 0:\n",
    "                f1 = 0\n",
    "            else:\n",
    "                f1 = (2 * (macro_pre * macro_rec) / zero_div)\n",
    "            return f1     # the score with micro averaging\n",
    "    else:\n",
    "        #\n",
    "        # Score of F1 to assign to list\n",
    "        f1_scores = []\n",
    "        #Calls for funcs.\n",
    "        prec_list = precision_score(y_true, y_pred, labels=None, average='None')\n",
    "        rec_list = recall_score(y_true, y_pred, labels=None, average='None')\n",
    "        dist = len(f1_scores)\n",
    "        for i in range (0, dist):\n",
    "            precision = prec_list[i]\n",
    "            recall = rec_list[i]\n",
    "            zero_div = precision + recall\n",
    "            if zero_div == 0:\n",
    "                f1 = 0\n",
    "            else:\n",
    "                f1 = (2 * (precs * recs) / zero_div)\n",
    "            f1_scores.append(f1)\n",
    "        return f1_scores     # the score with micro averaging          # list of score of each label \n",
    "    \n",
    "\n",
    "def all_metrics(y_true, y_pred, labels=None, average=None):\n",
    "    # you can compare you implementation with these\n",
    "    return (\n",
    "        precision_score(y_true, y_pred, labels=labels, average=average),\n",
    "        recall_score(y_true, y_pred, labels=labels, average=average),\n",
    "        f1_score(y_true, y_pred, labels=labels, average=average),\n",
    "        accuracy_score(y_true, y_pred)\n",
    "    )\n",
    "    # remove the likes above and use the function calls below: \n",
    "    #return (\n",
    "       # precision_score(y_true, y_pred, labels=labels, average=average),\n",
    "        #recall_score(y_true, y_pred, labels=labels, average=average),\n",
    "        #f1_score(y_true, y_pred, labels=labels, average=average),\n",
    "        #accuracy_score(y_true, y_pred)\n",
    "    #)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              |       |         macro       |         micro\n",
      "  model name  |  acc  | preci  recal    f1  | preci  recal    f1\n",
      "----------------------------------------------------------\n",
      "Perceptron    | 93.66 | 86.17  88.79  87.46 | 93.66  93.66  93.66\n",
      "Unigram       | 93.30 | 86.11  94.57  90.14 | 93.30  93.30  93.30\n",
      "Bigram        | 94.40 | 87.36  94.11  90.61 | 94.40  94.40  94.40\n",
      "Trigram       | 94.54 | 87.53  94.48  90.87 | 94.54  94.54  94.54\n",
      "Bigram-backoff| 24.63 | 87.72  31.96  46.85 | 24.63  24.63  24.63\n"
     ]
    }
   ],
   "source": [
    "models_preds = dict()\n",
    "print(\"              |       |         macro       |         micro\")\n",
    "print(\"  model name  |  acc  | preci  recal    f1  | preci  recal    f1\")\n",
    "print(\"-\"*58)\n",
    "for model_name, model in models.items(): #We will use this data from models() to output ate rin matrix\n",
    "    tags_pred = [\n",
    "        map_tag(\"en-ptb\", \"universal\", tag) if model_name == \"Perceptron\" else map_tag(\"en-brown\", \"universal\", tag)\n",
    "        for sent in test_set_sents\n",
    "        for word, tag in model.tag(sent)\n",
    "    ]\n",
    "    models_preds[model_name] = tags_pred\n",
    "    # print the results\n",
    "    \n",
    "    precision_macro, recall_macro, f1score_macro, accuracy = all_metrics(tags_true, tags_pred, average='macro')\n",
    "    precision_micro, recall_micro, f1score_micro, _ = all_metrics(tags_true, tags_pred, average='micro')\n",
    "    print(f\"{model_name:14}| {100*accuracy:5.2f} | {100*precision_macro:5.2f}  {100*recall_macro:5.2f}  {100*f1score_macro:5.2f} | {100*precision_micro:5.2f}  {100*recall_micro:5.2f}  {100*f1score_micro:5.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Perceptron\n",
      "\n",
      "tag\tprecision\trecall\tf1-score\n",
      "--------------------------------------------------\n",
      "==================================================\n",
      "==================================================\n",
      "Unigram\n",
      "\n",
      "tag\tprecision\trecall\tf1-score\n",
      "--------------------------------------------------\n",
      "==================================================\n",
      "==================================================\n",
      "Bigram\n",
      "\n",
      "tag\tprecision\trecall\tf1-score\n",
      "--------------------------------------------------\n",
      "==================================================\n",
      "==================================================\n",
      "Trigram\n",
      "\n",
      "tag\tprecision\trecall\tf1-score\n",
      "--------------------------------------------------\n",
      "==================================================\n",
      "==================================================\n",
      "Bigram-backoff\n",
      "\n",
      "tag\tprecision\trecall\tf1-score\n",
      "--------------------------------------------------\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for model_name, tags_pred in models_preds.items(): #and we will iterate this data and output it\n",
    "    print('='*50)\n",
    "    print(model_name) \n",
    "    print('')\n",
    "    precisions, recalls, f1scores, _ = all_metrics(tags_true, tags_pred, labels=tagset)\n",
    "    print(\"tag\\tprecision\\trecall\\tf1-score\")\n",
    "    print(\"-\"*50)\n",
    "    for tag, precision, recall, f1score in zip(tagset, precisions, recalls, f1scores):\n",
    "        print(f\"{tag}\\t{100*precision:9.2f}\\t{100*recall:6.2f}\\t{100*f1score:8.2f}\")\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron\n",
      "\n",
      "     |                        C         N         P         V      |\n",
      "     |         A    A    A    O    D    O    N    R    P    E      |\n",
      "     |         D    D    D    N    E    U    U    O    R    R      |\n",
      "     |    .    J    P    V    J    T    N    M    N    T    B    X |\n",
      "-----+-------------------------------------------------------------+\n",
      "   . |<1226>   1    .    .    .    .    .    .    .    2    .    . |\n",
      " ADJ |    . <638>   1   13    .    .  120   18    .    .    5    . |\n",
      " ADP |    .    2<1359>   8    .    5    .    .    .  119    7    . |\n",
      " ADV |    .   14   12 <302>   1    2    .    .    1    2    1    1 |\n",
      "CONJ |    .    .    .    . <253>   1    .    .    .    .    .    . |\n",
      " DET |    .    .    1    1    .<1265>   1    .  102    .    .    . |\n",
      "NOUN |    .   64    .    2    .    .<3464>   7    1    .   22    1 |\n",
      " NUM |    .    4    .    .    .    .    . <206>   .    .    .    . |\n",
      "PRON |    .    .    1    .    .    7    .    . <273>   .    .    . |\n",
      " PRT |    .    .    9    8    .   38    7    .    . <214>   1    . |\n",
      "VERB |    .   44    .    2    .    .   81    .    .    1<1766>   . |\n",
      "   X |    .    .    .    .    .    .    2    .    .    .    .   <2>|\n",
      "-----+-------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Unigram\n",
      "\n",
      "     |                        C         N         P         V      |\n",
      "     |         A    A    A    O    D    O    N    R    P    E      |\n",
      "     |         D    D    D    N    E    U    U    O    R    R      |\n",
      "     |    .    J    P    V    J    T    N    M    N    T    B    X |\n",
      "-----+-------------------------------------------------------------+\n",
      "   . |<1229>   .    .    .    .    .    .    .    .    .    .    . |\n",
      " ADJ |    . <690>   .   42    .    .   29    .    .    .    5   29 |\n",
      " ADP |    .    .<1367>   6    .    .    .    .    .  125    1    1 |\n",
      " ADV |    .   21   17 <293>   .    2    .    .    .    2    .    1 |\n",
      "CONJ |    .    .    .    . <254>   .    .    .    .    .    .    . |\n",
      " DET |    .    .    5    .    1<1364>   .    .    .    .    .    . |\n",
      "NOUN |    .   41    .    2    .    1<3226>   1    1    .   59  230 |\n",
      " NUM |    .    .    .    .    .    .    . <195>   .    .    .   15 |\n",
      "PRON |    .    .    8    .    .    .    .    . <273>   .    .    . |\n",
      " PRT |    .    2    7    1    .    .    .    .    . <267>   .    . |\n",
      "VERB |    .    5    1    2    .    .   91    .    .    .<1764>  31 |\n",
      "   X |    .    .    .    .    .    .    .    .    .    .    .   <4>|\n",
      "-----+-------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Bigram\n",
      "\n",
      "     |                        C         N         P         V      |\n",
      "     |         A    A    A    O    D    O    N    R    P    E      |\n",
      "     |         D    D    D    N    E    U    U    O    R    R      |\n",
      "     |    .    J    P    V    J    T    N    M    N    T    B    X |\n",
      "-----+-------------------------------------------------------------+\n",
      "   . |<1229>   .    .    .    .    .    .    .    .    .    .    . |\n",
      " ADJ |    . <720>   1   16    .    .   26    .    .    1    2   29 |\n",
      " ADP |    .    2<1417>   6    .    1    .    .    7   65    1    1 |\n",
      " ADV |    .   15   14 <302>   1    2    .    .    .    1    .    1 |\n",
      "CONJ |    .    .    .    . <253>   1    .    .    .    .    .    . |\n",
      " DET |    .    .    2    .    1<1367>   .    .    .    .    .    . |\n",
      "NOUN |    .   28    .    .    .    .<3269>   1    1    .   33  229 |\n",
      " NUM |    .    .    .    .    .    .    . <195>   .    .    .   15 |\n",
      "PRON |    .    .    7    .    .    .    .    . <274>   .    .    . |\n",
      " PRT |    .    2   55    6    .    .    .    .    . <214>   .    . |\n",
      "VERB |    .    3    .    1    .    .   48    .    .    .<1811>  31 |\n",
      "   X |    .    .    .    .    .    .    .    .    .    .    .   <4>|\n",
      "-----+-------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Trigram\n",
      "\n",
      "     |                        C         N         P         V      |\n",
      "     |         A    A    A    O    D    O    N    R    P    E      |\n",
      "     |         D    D    D    N    E    U    U    O    R    R      |\n",
      "     |    .    J    P    V    J    T    N    M    N    T    B    X |\n",
      "-----+-------------------------------------------------------------+\n",
      "   . |<1229>   .    .    .    .    .    .    .    .    .    .    . |\n",
      " ADJ |    . <718>   1   18    .    .   26    .    .    1    2   29 |\n",
      " ADP |    .    2<1414>   5    .    4    .    .    9   63    2    1 |\n",
      " ADV |    .   15   13 <303>   .    2    .    .    .    1    1    1 |\n",
      "CONJ |    .    .    1    . <253>   .    .    .    .    .    .    . |\n",
      " DET |    .    .    2    .    1<1366>   .    .    1    .    .    . |\n",
      "NOUN |    .   25    .    .    .    .<3274>   1    1    .   31  229 |\n",
      " NUM |    .    .    .    .    .    .    . <195>   .    .    .   15 |\n",
      "PRON |    .    .    4    .    .    .    .    . <277>   .    .    . |\n",
      " PRT |    .    2   49    3    .    .    .    .    . <223>   .    . |\n",
      "VERB |    .    4    .    1    .    .   43    .    .    .<1815>  31 |\n",
      "   X |    .    .    .    .    .    .    .    .    .    .    .   <4>|\n",
      "-----+-------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Bigram-backoff\n",
      "\n",
      "     |                        C         N         P         V      |\n",
      "     |         A    A    A    O    D    O    N    R    P    E      |\n",
      "     |         D    D    D    N    E    U    U    O    R    R      |\n",
      "     |    .    J    P    V    J    T    N    M    N    T    B    X |\n",
      "-----+-------------------------------------------------------------+\n",
      "   . | <259>   .    .    .    .    .    .    .    .    .    .  970 |\n",
      " ADJ |    . <171>   .    6    .    .    5    .    .    .    .  613 |\n",
      " ADP |    .    . <331>   .    .    .    .    .    2   20    . 1147 |\n",
      " ADV |    .    2    3 <107>   .    .    .    .    .    .    .  224 |\n",
      "CONJ |    .    .    .    .  <42>   1    .    .    .    .    .  211 |\n",
      " DET |    .    .    .    .    . <480>   .    .    .    .    .  890 |\n",
      "NOUN |    .    9    .    .    .    . <750>   .    .    .    5 2797 |\n",
      " NUM |    .    .    .    .    .    .    .  <49>   .    .    .  161 |\n",
      "PRON |    .    .    1    .    .    .    .    . <123>   .    .  157 |\n",
      " PRT |    .    .   12    1    .    .    .    .    .  <56>   .  208 |\n",
      "VERB |    .    .    .    .    .    .    7    .    .    . <512>1375 |\n",
      "   X |    .    .    .    .    .    .    .    .    .    .    .   <4>|\n",
      "-----+-------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add confusion matrix here CM taken from nltk site\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "for model_name, tags_pred in models_preds.items():\n",
    "    print(model_name + '\\n')\n",
    "    ref = tags_true \n",
    "    test = tags_pred\n",
    "    CM = ConfusionMatrix(ref, test)\n",
    "    print(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backoff\n",
    "\n",
    "Compare the result of the bigram models (with and without backoff). In one or two sentences, explain why backoff is important for the n-gram models? Why tagging works better with backoff?\n",
    "\n",
    "For more details check out the definition of backoff in [Chapter 3 section 3.4.3](https://web.stanford.edu/~jurafsky/slp3/3.pdf) (Jurafsty and Martin; 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can estimate the probabilities of unseen n-grams instead of throwing us errors when we test larger n-grams we can use Backoff which includes\n",
    "#smoothing, so say we want a trigram for rare events... \n",
    "#Good this about Backoof, it uses less context if bigram, unigram etc is sufficient enough hence \"back\"...\"off\".\n",
    "#Less context in this case is better for the things the model hasnt learned yet. Consequently this means taht it will have a bigger dataset which\n",
    "#will lead to less mistakes when tagging. Therefore it is important since it make up for the insuffiecny of the rare cases.\n",
    "#Comparing the results (Bigram and Bigram-backoff) we can see that \"no backoff\" has good enough precision score but a very low recall\n",
    "#score thus it will give higher count of false negatives.\n",
    "\n",
    "#So shorter answear:\n",
    "#If zero occurences of N-gram equls true when not using backoff, the proability will also be zero and no tag can be assigned.\n",
    "#Therefore not tag can be correctly predicted for some rare N-grams.\n",
    "#With backoff, if there is an instance of rare N-gram whcih hasnt been seen it will \"backoff\" to N-1 -gram and \n",
    "#then we will still get a chance to guess the crrect label even if specific bi-gram hasnt been seen. So it \n",
    "#fall to lesser context but still has a valid guessing spektrum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
